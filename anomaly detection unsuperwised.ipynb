{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t2s2AK6Tou3K",
        "outputId": "78b25d97-ca0f-48a6-ba4c-c26a9b939474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts of anomalies and normal emails:\n",
            "Anomaly_Label\n",
            "normal     5156\n",
            "anomaly     416\n",
            "Name: count, dtype: int64\n",
            "Sample of anomalous emails:\n",
            "      Category                                  Processed_Message  \\\n",
            "4340         0  ['got', 'outta', 'class', 'gon', 'na', 'go', '...   \n",
            "5029         0  ['go', 'chase', 'run', 'shes', 'crossing', 'st...   \n",
            "1373         0  ['1', 'go', 'write', 'msg', '2', 'put', 'dicti...   \n",
            "2377         0  ['im', 'way', 'home', 'went', 'change', 'batt'...   \n",
            "2230         0  ['haha', 'money', 'leh', 'later', 'got', 'go',...   \n",
            "\n",
            "     Anomaly_Label  \n",
            "4340       anomaly  \n",
            "5029       anomaly  \n",
            "1373       anomaly  \n",
            "2377       anomaly  \n",
            "2230       anomaly  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MaxAbsScaler\n",
        "from sklearn.ensemble import IsolationForest\n",
        "\n",
        "# Read the CSV file\n",
        "df = pd.read_csv('lemmatized_without_stopwords_puntuation_libraries.csv')\n",
        "\n",
        "# Convert text to numeric features using TF-IDF vectorization\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.95, min_df=0.05, ngram_range=(1, 2))  # Adjust parameters as needed\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(df['Processed_Message'])\n",
        "\n",
        "# Scale the TF-IDF features using MaxAbsScaler\n",
        "scaler = MaxAbsScaler()\n",
        "tfidf_matrix_scaled = scaler.fit_transform(tfidf_matrix)\n",
        "\n",
        "# Fit the Isolation Forest model\n",
        "isolation_forest = IsolationForest(n_estimators=100, contamination=0.1, random_state=42)\n",
        "isolation_forest.fit(tfidf_matrix_scaled)\n",
        "\n",
        "# Predict anomalies\n",
        "anomalies = isolation_forest.predict(tfidf_matrix_scaled)\n",
        "\n",
        "# Assign anomaly labels to the DataFrame\n",
        "df['Anomaly'] = anomalies\n",
        "\n",
        "# Map numeric labels to meaningful labels\n",
        "label_mapping = {1: 'normal', -1: 'anomaly'}\n",
        "df['Anomaly_Label'] = df['Anomaly'].map(label_mapping)\n",
        "\n",
        "# Drop the 'Anomaly' column\n",
        "df = df.drop(columns=['Anomaly'])\n",
        "\n",
        "# Count anomalies and normal emails\n",
        "anomaly_counts = df['Anomaly_Label'].value_counts()\n",
        "print(\"Counts of anomalies and normal emails:\")\n",
        "print(anomaly_counts)\n",
        "\n",
        "# Display a sample of anomalous emails\n",
        "print(\"Sample of anomalous emails:\")\n",
        "print(df[df['Anomaly_Label'] == 'anomaly'].sample(5))  # Display 5 random anomalous emails\n",
        "\n",
        "# Optionally, save the results to a new CSV file\n",
        "df.to_csv('emails_with_anomalies.csv', index=False)\n"
      ]
    }
  ]
}